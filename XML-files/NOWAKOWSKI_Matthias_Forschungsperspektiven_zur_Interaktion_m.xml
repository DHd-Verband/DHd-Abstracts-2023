<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:id="NOWAKOWSKI_Matthias_Forschungsperspektiven_zur_Interaktion_m" xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Forschungsperspektiven zur Interaktion mit Musiknotation</title>
                <author>
                    <persName>
                        <surname>Nowakowski</surname>
                        <forename>Matthias</forename>
                    </persName>
                    <affiliation>Center of Music and Film Informatics</affiliation>
                    <email>matthias.nowakowski@hfm-detmold.de</email>
                </author>
                <author>
                    <persName>
                        <surname>Berndt</surname>
                        <forename>Axel</forename>
                    </persName>
                    <affiliation>Center of Music and Film Informatics</affiliation>
                    <email>axel.berndt@th-owl.de</email>
                </author>
                <author>
                    <persName>
                        <surname>Hadjakos</surname>
                        <forename>Aristotelis</forename>
                    </persName>
                    <affiliation>Center of Music and Film Informatics</affiliation>
                    <email>aristotelis.hadjakos@hfm-detmold.de</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2022-07-15T10:28:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                    <publisher>Culture and Computation Lab</publisher>
                    <address>
                        <addrLine>Université du Luxembourg</addrLine>
                        <addrLine>2, Avenue de l'Université</addrLine>
                        <addrLine>L-4365 Esch-sur Alzette</addrLine>
                        <addrLine>Luxembourg</addrLine>
                    </address>
                    <publisher>Luxembourg Centre for Contemporary and Digital History</publisher>
                    <address>
                        <addrLine>Université du Luxembourg</addrLine>
                        <addrLine>2, Avenue de l'Université</addrLine>
                        <addrLine>L-4365 Esch-sur Alzette</addrLine>
                        <addrLine>Luxembourg</addrLine>
                    </address>
                    <publisher>Trier Center for Digital Humanities</publisher>
                    <address>
                        <addrLine>Universität Trier</addrLine>  
                        <addrLine>Universitätsring 15</addrLine>
                        <addrLine>54296 Trier</addrLine>
                        <addrLine>Deutschland</addrLine>
                    </address>
                </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Vortrag</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>Musiknotation</term>
                    <term>Interaktion</term>
                    <term>Human Computer Interaction</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>Schreiben</term>
                    <term>Interaktion</term>
                    <term>Multimedia</term>
                    <term>Notenblätter</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <div type="div1" rend="DH-Heading1">
                <head>Startpunkt Notensatzprogramme</head>
                <p style="text-align: justify;"> </p>
                <p style="text-align: justify;">Musikalische Notation ist das vordringliche Medium zur Musiküberlieferung, aber auch der alltäglichen musikalischen Praxis und ist durch ihre Textualität auch Gegenstand der Digital Humanities. Die wissenschaftliche Beschäftigung mit Musik wie auch die Kommunikation über Musik und musikalische Ideen ist in weiten Teilen der heutigen Musikkultur ohne eine Notenschrift nicht denkbar. Selbst die Interaktion zwischen Musizierenden und mit dem musikalischen Material findet vorrangig vermittels dieses Mediums statt. Dabei kann die Verschriftlichung von Musik unterschiedliche Formen annehmen, sei es die sogenannte Common Western Music Notation (CWMN), Notenschriften aus anderen Kulturen oder zeitlichen Epochen, die vielfältigen eher technischen Darstellungsformen in Software-Werkzeugen zur Musikproduktion oder die analytischen Visualisierungen aus dem Bereich des Music Information Retrieval (Khulusi 2020).</p>
                <p style="text-align: justify;">Trotz dieser Vielfalt gilt die Interaktion mit Musiknotation aber noch immer primär als Domäne klassischer Notensatzprogramme<ref n="1" target="ftn1"/> und wird jenseits dessen kaum tiefergehend thematisiert. Die Funktionen von Notensatzprogrammen umfassen in erster Linie die Eingabe und das Editieren von Notenmaterial nach der CWMN sowie das Platzieren im Layout für den Druck. Sie sind damit Textbearbeitungsprogrammen nicht unähnlich. Ihre Benutzeroberflächen folgen ähnlichen Gestaltungskonzepten, visualisieren das interaktive Notenblatt im Bildzentrum und umgeben es mit vielfältigen Werkzeugpaletten und Modusschaltern, z.B. um Noten zu schreiben, editieren und abzuspielen. Es sind klassische WYSIWYG<ref n="2" target="ftn2"/>-Desktopanwendungen, die für eine filigrane Maus- und Tastatursteuerung konzipiert sind. Diese etablierten Konventionen der Interaktion in einer Desktopumgebung stoßen bei den vielfältigen Anwendungsszenarien von Musiknotation jedoch an ihre Grenzen. Dies geht über das bloße Erstellen, Anzeigen und Ausdrucken der Noten hinaus. Der Abschnitt „Vielfalt der Anwendungsszenarien“ wird dieses Spektrum überblickshaft umreißen. Abschnitt 3 „Interaktion jenseits von Notensatz“ wird den Fokus entsprechend weiten und Interaktionskonzepte und Technologien aus angrenzenden Forschungs- und Anwendungsgebieten im Bereich Musik in den Blick nehmen. Leere Flecken auf der „Forschungslandkarte“ werden im Abschnitt „Bestimmung des Design Space“ herausgearbeitet und anhand von Beispielen verdeutlicht.
                </p>
                <p style="text-align: justify;"> </p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Vielfalt der Anwendungsszenarien</head>
                <p style="text-align: justify;">Die heute etablierten graphischen Darstellungs- und Interaktionsformen für Musik möchten wir zunächst nach dem Anwendungsbezug wie folgt systematisieren und ihre jeweiligen Besonderheiten herausarbeiten: 
                    <hi rend="italic">Erstellung</hi>, 
                    <hi rend="italic">Musizieren</hi>, 
                    <hi rend="italic">Präsentation</hi> und 
                    <hi rend="italic">Gaming</hi>. 
                </p>
                <p style="text-align: justify;">
                    <hi rend="italic">Erstellung:</hi> Neben der manuellen Noteneingabe spielt der Notenscan eine immer wichtigere Rolle. Solche Scans werden mittels Optical Music Recognition-Verfahren in editierbaren Notentext übersetzt (Calvo-Zaragoza 2020). In der historisch-kritischen Musikedition liegen der Erstellung meist mehrere Quellen zu Grunde, die nicht nur (digital) erfasst, sondern auch verglichen werden. Werkzeugen für vergleichende Ansichten kommt dabei eine zentrale Rolle zu (Kepper 2007, Waloschek 2019). Grundsätzlich fanden diese Arbeitsprozesse seitens der Interaktionsforschung bislang kaum Beachtung.
                </p>
                <p style="text-align: justify;">Es ist vor allem aber die Kreativarbeit, welche von spontaneren und direkteren Eingabemöglichkeiten zur Erstellung profitieren würde. Beim Komponieren entstehen Skizzen nicht notwendigerweise nur am Schreibtisch oder Klavier, sondern in Alltagssituationen, in denen allenfalls ein Smartphone gerade zur Hand ist. Bei Größeren Werken geht oft eine Planung der Formteile und ihrer Proportionen voraus, die dann in beliebiger Reihenfolge oder auch parallel ausgearbeitet werden. Notensatzprogramme können solchen nichtlinearen, kreativen Prozessen aber kaum gerecht werden.</p>
                <p style="text-align: justify;">
                    <hi rend="italic">Musizieren:</hi> Im Ensemble spielen die Musizierenden oft nicht aus der Partitur, sondern aus Einzelstimmen, und sehen damit nicht, was ihre Mitmusizierenden spielen. In der Bandmusik und in improvisatorischen Kontexten reduziert sich auch das eigene Notenmaterial weiter auf Lead Sheets oder nur mündliche Absprachen zu Akkordfolgen und Tempo. Die Kommunikation beim Ensemblespiel geschieht über den vermittelnden Dirigenten, Sichtkontakt, Bewegungsgesten und das eigene Gehör. Das Notenmaterial ist in dieser Konstellation allerdings kein starres Objekt, das nur noch gelesen wird. Das wird vor allem bei den zahlreichen Eintragungen der Musizierenden deutlich: Interpretationsanweisungen, Hinweise zur Kommunikation, Ergänzungen und Veränderungen von Noten. Bei vernetzten, digitalen Noten könnten diese auch mit den anderen geteilt werden. Maus und Tastatur sind in diesem Szenario nicht praktikabel. Für das Weiterblättern kommen daher Pedale und Taster zum Einsatz. Im Idealfall hört das Gerät sogar mit, führt ein Audio-to-Score Alignment aus und blättert vollautomatisch weiter.
                </p>
                <p style="text-align: justify;">Auf der Seite der Musikproduktion werden Eintragungen in Partituren gezeichnet und dienen dazu, Takes im musikalischen Kontext zu verorten, gelungene oder weniger gelungene Stellen zu annotieren und den Schnittplan zu erstellen. Dabei interagieren die Noten mit den Aufnahmedaten in einer Digital Audio Workstation (DAW) (Waloschek 2017).</p>
                <p style="text-align: justify;">
                    <hi rend="italic">Gaming</hi>: Visualisierungen sind oft stilisierte und abstrahierte Ableitungen klassischer Musiknotation. Diese können, ebenso wie die erklingende Musik, fest vorgegeben und unveränderlich sein, wobei sie dann Tempo und Rhythmus von Geschicklichkeitsübungen diktieren. Sie können aber auch interaktiv sein, sodass die Spielenden durch ihre Interaktion Einfluss auf die Musik nehmen, sie spielend erzeugen oder arrangieren (Berndt 2011).
                </p>
                <p style="text-align: justify;">
                    <hi rend="italic">Präsentation:</hi> Dies zielt vor allem darauf ab, neue Zugänge zum Verständnis der Musik zu schaffen. In den Anwendungsgebieten Musikwissenschaft, Music Information Retrieval und Musikvermittlung gilt die Interaktion daher vor allem der Annotation von Analyseergebnissen. In Videos werden Notentext und klingende Musik synchron dargestellt, um ein mitlesendes Hören zu ermöglichen. Einige YouTube-Kanäle inszenieren ihre Musikanalysen in Form aufwendig angefertigter, annotierter Partiturreduktionen<ref n="3" target="ftn3"/>. Im schulischen und akademischen Musikunterricht findet sich das Notenbild großformatig auf der (digitalen) Tafel wieder, wo kurze Notentexte verfasst, editiert, angehört und nachgespielt werden. Da meist klassische Notensatzprogramme an die Wand projiziert werden, fällt auf, dass hierfür immer wieder zu Maus und Tastatur gegriffen werden muss, also nicht mit dem Tafelbild direkt interagiert wird. In rein virtuellen und hybriden Unterrichtsformen betreiben die Beteiligten jeweils lokal ein Notensatzprogramm, können zwar Bild und Ton mit den anderen teilen, nicht aber gemeinsam am selben Notentext arbeiten. Hierfür lohnt ein Blick in Museen, wo musikbezogene Medien zumeist von mehreren Besuchern gleichzeitig erlebt und bedient werden können, sei es an großen Multitouch Displays, Tabletops, mittels Freihandgesteninteraktion (Berndt 2016) oder in raumgreifenden Klanginstallationen (Berndt 2019).
                </p>
                <p style="text-align: justify;">Die meisten der hier aufgeführten Anwendungsszenarien erfordern aber mehr als nur neue Funktionen innerhalb der etablierten und durch ihre Konventionen geprägten Notensatzprogramme. Sie erfordern ein unverstelltes Neudenken von interaktiven Zugängen zum Medium Notentext. </p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Interaktion jenseits von Notensatz</head>
                <p style="text-align: justify;"> </p>
                <p style="text-align: justify;">Interaktionen mit Musiknotation lassen sich nur schwer von ihren musikalischen Realisationen trennen. Daher überrascht es nicht, dass die Exploration von neuen Eingabemodalitäten vor allem in der künstlerischen Forschung geschieht. Hier sind meist die schnelle und unmittelbare Eingabe von Noten Hauptaspekt der Betrachtung. Der Fokus auf die künstlerische Forschung konnte vor allem durch eine Institutionalisierung der „New Interfaces for Musical Expression“ (NIME) geschehen. In einer Analyse der Texte der gleichnamigen Konferenzen der letzten 20 Jahre zeigt sich, dass diese vor allem neuartigen Musikinstrumenten, Aufführungen und elektronischen Kontrollmöglichkeiten gewidmet sind (Fasciani 2021). Viele dieser Controller, Software und Interaktionsformen sind jedoch meist schwer generalisierbar und werden nur durch ein Werk oder eine Menge an Werken desselben Künstlers exemplifiziert. Solche Formen sind etwa die Synthese von Noten durch Sensoren am Körper oder Objekte, wie etwa Taktstöcke. Dadurch können aus einer vorher festgelegten Geste Notationen entstehen, die dann von Maschine oder Musizierenden umgesetzt werden. Eine Geste wird übersetzt in musikalische Parameter, wie z.B. Tonhöhe, Dynamik etc., und ermöglichen abstraktere Interaktionen als Notensatzprogramme es erfordern. Es entstehen live notierte Abschnitte, welche als eine Reihe von musikalischen Gesten dargestellt werden. Dabei macht es keinen Unterschied, ob diese Notationen in Form von CWMN oder anderer graphischer Elemente dargestellt werden (Frame 2022). Darüber hinaus sind aktionsbasierte Notationen gestische Anweisungen, die so dargestellt werden, dass sie direkt und ohne Kenntnis von anderen Notationssystemen ausgeführt werden können. Sie werden im Voraus erstellt und ähneln in ihrem Mapping Tabulaturen, erweitern sie aber durch stilisierte Animationen von Hand- oder Körperbewegungen, welche den entsprechenden Klang erzeugen sollen (Clay 2010). Diese Bewegungen können elektronisch analysiert werden, um z.B. automatisch „weiterzublättern“ (Dori 2020).</p>
                <p style="text-align: justify;">Als Erweiterung der sensorbedingten Interaktion kann man die Nutzung von Virtual Reality (VR) verstehen. Die Immersion der Nutzenden geschieht durch die Übertragung der körperlichen Bewegung in einen simulierten, dreidimensionalen Raum. Bewegungen können in den realen Raum abgebildet werden und können, z.B. als Nachzeichnungen dieser Bewegungen, Ausgang einer Notation für Musiker*innen sein (Santini 2022). Dafür benötigt es u.a. Controller, welche das Greifen und Zeigen ersetzen. Ebenso muss die Körperposition verfolgt werden können, um eine Beziehung zu den simulierten Objekten herzustellen. Solches Greifen wird auch in Anwendungen zu Lernszenarien mit haptischen und räumlichen Komponenten genutzt, in welchen Nutzer Noten oder Harmonieverläufe direkt vertikal und horizontal anordnen und so allein durch die Kopfbewegung einen direkten Überblick über das Geschaffene gewinnen können (Shvets 2022).</p>
                <p style="text-align: justify;">Im Unterschied dazu wird in der Augmented Reality (AR) kein Raum simuliert, sondern Informationen im realen Raum ergänzt. Neben spezialisierten Geräten, wie der HoloLens,<ref n="4" target="ftn4"/> sind auch viele Smartphones in der Lage, das Livekamerabild zu analysieren und mit Augmentierungen anzureichern. Beispiele für die AR-Nutzung gibt es im Bereich der Instrumentallehre. Darstellung der Noten können als Pianorolle direkt über der Klaviatur dargestellt werden und ermöglicht so das gleichzeitige Beobachten von Notation, Fingern und Instrument (Kim-Boyle 2022).
                </p>
                <p style="text-align: justify;">Jenseits vom Notensatz erhält die Notation also immer mehr räumliche Bedeutung. Durch Web-Technologien wird auch die Entfernung zwischen den Zusammenspielenden beliebig. So können etwa durch Interaktionen der Dirigierenden Notationen direkt auf Displays im Orchester verteilt werden (Andersen 2022), oder es ermöglicht den Musizierenden sich über verschiedene Geräte sowohl in Proben- als auch in Aufführungssituationen zu synchronisieren (Bell 2017, Bell 2021).</p>
                <p style="text-align: justify;">Die realweltliche Interaktion mit dem Notenblatt auf dem Notenpult findet über Handgesten (z.B. Umblättern des Notenblattes, Zeigen) und Stift (z.B., Schreiben von Noten, Ergänzen von Vortragsanweisungen) statt. Vor diesem Hintergrund kommt auch der mittlerweile sehr umfangreichen Forschung zu Touch- und Stiftinteraktion - und im genannten Nutzungsszenario insbesondere auf Tablets - eine große Relevanz zu (Baró 2019).</p>
                <p style="text-align: justify;">In den Besprechungen der NIMEs werden allerdings nichtkünstlerische Bereiche oft vernachlässigt. So werden z.B. in der Pädagogik musikbezogene Interfaces auch als Mittel genutzt, um das Lernen in anderen Domänen zu erleichtern. Das „Computational Music Thinking“ (Repenning 2019) ist ein Ansatz, um informatische und musikalische Konzepte zu verknüpfen und Wissen aus dem jeweils anderen Bereich zu übertragen. Grundlegend dafür ist die Überzeugung, dass musikalische und programmatische Muster (
                    <hi rend="italic">Patterns</hi>) übersetzbar seien. Daraus entstehen Programmierumgebungen, die durch die Abstraktion der musikalischen Elemente eher visuellen Programmiersprachen, wie Max/MSP<ref n="5" target="ftn5"/> ähneln. Die Interaktion ist hier aber klar auf Maus und Tastatur ausgerichtet.
                </p>
                <p style="text-align: justify;">Als Ziel der Interaktionsforschung sticht also deutlich die künstlerische Aufführung hervor. Für deren Verwendung in Forschungsbereichen der Digital Humanities möchten wir sie in einem möglichen Design Space einbetten. </p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Bestimmung des Design Space</head>
                <p style="text-align: justify;">Als Design Space verstehen wir den Raum, in dem die Elemente vorhanden sind, die für die Konstitution eines Forschungsbereiches herangezogen werden können. Der Raum sei hier aber nicht nur als Metapher verstanden. Als Instrument der Positionsbestimmung im Design Space kann die Kombination der Elemente als Vektor dargestellt werden. Abbildung 1 zeigt nur eine flache Darstellung der verschiedenen Dimensionen, welche in verschiedenen Farben kodiert sind. </p>
                <p style="text-align: justify;">Die Auswahl der Elemente basiert auf der Analyse von Interaktionsformen mit musikalischer Notation. Die Dimensionen werden definiert durch die Objekte der Interaktion, der Granularität ihrer Interaktionsgegenstände, des musikalischen und sozialen Kontexts der Nutzung, sowie Fragen nach verschiedenen Darstellungsformen und Interaktionsgeräten. Diese Auflistung ist beileibe nicht vollständig, geben aber eine Orientierung und Klassifikation aktueller und zukünftiger Forschungsfelder.</p>
                <p style="text-align: justify;">In klassischen Musiknotationsprogrammen interagieren die Nutzenden mit Notationszeichen auf der logischen Ebene nach der Klassifikation von Maxwell (Maxwell 1981): Es können nur syntaktisch wohlgeformte Notationen erstellt werden. Die kleinste Granularität der Interaktion ist hier also die Manipulation von Musikzeichen auf der logischen Ebene. In handgeschriebenen Notationen finden sich allerdings manchmal Verletzungen dieser „Logik“, zum Beispiel durch Auslassungen, abgekürzte Notationen oder absichtliche Überschreitungen. Daher erlaubt das bei digitalen historisch-kritischen Ausgaben häufig benutzte MEI-Format (Hankinson 2011) die Modellierung von Inhalten u.a. auf der grafischen Ebene, um damit auch Merkmale von Handschriften nachbilden zu können. Eine weitere Ebene tiefer kann es auch von Interesse sein, mit den grafischen Grundelementen des Notentextes als Vektorgrafik zu interagieren, um besondere gestalterische Ziele umzusetzen. Andererseits ist es auch sinnvoll mit Notentexten auf höheren Granularitätsstufen zu interagieren: Beispielsweise werden in digitalen Musikeditionen häufig verschiedenen Quellen taktweise verlinkt, um Ähnlichkeiten und Abweichungen zu erkennen. Interfaces sollten auf unterschiedlichen Stufen bedienbar sein, unter anderem um graphische Primitive, Musikzeichen auf der grafischen oder logischen Ebene, Takte, Stimmen, von Anwender*innen ausgewählte Teile bis hin zu Musiksammlungen zu manipulieren.</p>
                <p style="text-align: justify;">Solche Interfaces werden auch in vielfältigen sozialen Kontexten verwendet, ob gemeinsam oder allein, beim Einstudieren eines Stückes, in einer Probe, bei der gemeinsamen Arbeit an einem Notentext oder im Musikunterricht. Auch der Musikstil hat einen Einfluss auf das Interface Design, insbesondere welche Darstellungsformen zum Einsatz kommen: Wenn improvisiert wird, könnten Lead Sheets zum Einsatz kommen. Bei Neuer Musik oder in NIME-Performances mit Controllern und Live-Elektronik, kommen meist grafische Partituren zum Einsatz. Piano Roll und auf Spektrogramm basierende Darstellungsformen können unabhängig von musikalischen Vorkenntnissen in verschiedenen Stilen genutzt werden.</p>
                <p style="text-align: justify;">Zuletzt spielen auch die eingesetzten Interaktionsgeräte und -technologien eine Rolle. Noch vor WIMP<ref n="6" target="ftn6"/>-basierten Interfaces für PCs und Laptops, sind heute Touch-Interfaces für Smartphones und Tablets das wichtigste Medium für den Umgang mit Noten, während Interfaces für AR/VR sich noch in einem experimentellen Stadium befinden.
                </p>
                <p style="text-align: justify;">Ein Beispiel: Die Elemente sind aus den jeweiligen Kategorien frei kombinierbar. So ließe sich z.B. ein Vektor erstellen, der alle fünf Kategorien umfasst: [AR, Sammlungen, Skizzenhaft, NIME, Lehre]. In diesem Fall könnte man sich eine AR-Anwendung (vielleicht auf einem Smartphone) vorstellen, welche auf vordefinierte Sammlungen angewandt werden könnte. Die Skizzen die daraus automatisiert erstellt werden, wären Grundlagen für Performances mit einem NIME, welche in einem Lehrkontext (im Musikunterreicht oder an einer Musikhochschule) besprochen werden. Beiträge dazu würden beispielsweise die kreative Aneignung von klassischer Musik durch AR erforschen. Verkleinert man den Vektor und lässt den sozialen Kontext „Lehre“ und das Interaktionsmedium „AR“ heraus, so hätte man eine Fokussierung auf Sammlung beispielsweise als Grundlage generativer Musik oder für die Nachmodellierung aktueller kompositorischer Prozesse. Bei der Aufführung bietet sich darüber hinaus eine Aufzeichnung als Forschungsgegenstand an, die selbst weiteren digitalen Analysen unterzogen werden kann. </p>
                <p>Mit diesem Paper haben wir die vielfältigen Perspektiven für Interaktionen mit Musiknotationen analysiert. Dabei haben wir auch gezeigt welches Innovationspotential hier noch offen liegt – ein lohnendes Feld für zukünftige Forschungen und Entwicklungen.</p>
                <p style="text-align: justify;">​​</p>
            </div>
        </body>
        <back>
<div type="notes">
<note rend="footnote text" xml:id="ftn1" n="1">
                         Zu den bekanntesten Vertretern zählen etwa MuseScore (Werner Schweer &amp; The MuseScore developer community 2002), Dorico (Steinberg 2022), Sibelius (Avid Technology, Inc. 1993), Finale (MakeMusic, Inc. 1988) und Capella (capella-software AG 1992).
                    </note>
<note rend="footnote text" xml:id="ftn2" n="2">
                         What You See Is What You Get.
                    </note>
<note rend="footnote text" xml:id="ftn3" n="3">
                         Z.B https://www.youtube.com/c/BigDaddyDave/videos (zugegriffen am 29. Juli 2022)
                    </note>
<note rend="footnote text" xml:id="ftn4" n="4">
                         https://www.microsoft.com/de-de/hololens (zugegriffen am 29. Juli 2022)
                    </note>
<note rend="footnote text" xml:id="ftn5" n="5">
                         https://cycling74.com/products/max (zugegriffen am 29. Juli 2022)
                    </note>
<note rend="footnote text" xml:id="ftn6" n="6">
                         Windows, Icons, Menus, Pointer
                    </note></div>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliographie</head>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Andersen, Drake</hi>. 2021. „Indra: A Virtual Score Platform for Networked Musical Performance“. In Proceedings of the International Conference on Technologies for Music Notation and Representation – TENOR’20/21, hg. von Rama Gottfried, Georg Hajdu, Jacob Sello, Alessandro Anatrini und John MacCallum, 227–234. Hamburg, Germany: Hamburg University for Music / Theater. https://doi.org/10.5281/zenodo.4764757
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Baró, Arnau, Pau Riba, Jorge Calvo-Zaragoza und Alicia Fornés.</hi> 2019. „From Optical Music Recognition to Handwritten Music Recognition: A Baseline“. 
                        <hi rend="italic">Pattern Recognition Letters</hi> 123: 1–8. https://doi.org/10.1016/j.patrec.2019.02.029
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Bell, Jonathan</hi>. 2021. „Distributed Notation in the Browser, an Overview“. In 
                        <hi rend="italic">Proceedings of the International Conference on Technologies for Music Notation and Representation – TENOR’20/21</hi>, hg. von Rama Gottfried, Georg Hajdu, Jacob Sello, Alessandro Anatrini und John MacCallum, 251–259. Hamburg, Germany: Hamburg University for Music / Theater. https://doi.org/10.5281/zenodo.4764764
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Bell, Jonathan, und Benjamin Matuszewski</hi>. 2017. „SMARTVOX. A Web-Based Distributed Media Player as Notation Tool for Choral Practices“. In 
                        <hi rend="italic">Proceedings of the International Conference on Technologies for Music Notation and Representation – TENOR’17</hi>, hg. von Helena Lopez Palma, Mike Solomon, Emiliana Tucci und Carmen Lage, 99–104. A Coruna, Spain: Universidade da Coruna. https://doi.org/10.5281/zenodo.924143
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Berndt, Axel.</hi> 2011. „Diegetic Music: New Interactive Experiences“. In 
                        <hi rend="italic">Game Sound Technology and Player Interaction: Concepts and Developments</hi>, hg. von M. Grimshaw, 60–76. Hershey, PA: IGI Global. http://dx.doi.org/10.4018/978-1-61692-828-5.ch004
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Berndt, Axel, Simon Waloschek und Aristotelis Hadjakos.</hi> 2016. „Hand Gestures in Music Production“. In 
                        <hi rend="italic">Proc. of the Int. Computer Music Conf. (ICMC)</hi>, hg. von H. Timmermans. Utrecht, The Netherlands: International Computer Music Association, HKU University of the Arts Utrecht, Gaudeamus Muziekweek.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Berndt, Axel, und Joachim Veit,</hi> Hrsg. 2019. Inside Beethoven! Das begehbare Ensemble - Begleitpublikation zur Klanginstallation der Hochschule für Musik Detmold zum Septett op. 20 und Trio op. 38 (mit CD). Bonn, Germany: Beethoven-Haus Bonn.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Calvo-Zaragoza, Jorge, Jan Hajič Jr und Alexander Pacha.</hi> 2020. „Understanding optical music recognition“. 
                        <hi rend="italic">ACM Computing Surveys (CSUR)</hi> 53, Nr. 4: 1–35. https://doi.org/10.48550/arXiv.1908.03608
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Clay, Arthur, und Jason Freeman.</hi> 2010. „Preface: Virtual Scores and Real-Time Playing“. 
                        <hi rend="italic">Contemporary Music Review</hi> 29, Nr. 1: 1–1. https://doi.org/10.1080/07494467.2010.509587
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold" xml:space="preserve">Dori, Gil. </hi> 2020. „Using Gesture Data to Generate Real-Time Graphic Notation: a Case Study“. In 
                        <hi rend="italic">Proceedings of the International Conference on Technologies for Music Notation and Representation – TENOR’20/21,</hi> hg. von Rama Gottfried, Georg Hajdu, Jacob Sello, Alessandro Anatrini und John MacCallum, 68–74. Hamburg, Germany: Hamburg University for Music / Theater.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Fasciani, Stefano, und Jackson Goode.</hi> 2021. „20 NIMEs: Twenty Years of New Interfaces for Musical Expression“. In 
                        <hi rend="italic">Proceedings of the International Conference on New Interfaces for Musical Expression</hi>. Shanghai, China. https://doi.org/10.21428/92fbeb44.b368bcd5
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Frame, Ciaran, Alon Ilsar und Sam Trolland.</hi> 2022. „Mutable Gestures: A New Animated Notation System for Conductor and Chamber Ensemble“. In 
                        <hi rend="italic">Proceedings of the International Conference on Technologies for Music Notation and Representation – TENOR’2022</hi>, hg. von Vincent Tiffon, Jonathan Bell und Charles de Paiva Santana, 1–7. Marseille, France: PRISM Laboratory.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Hankinson, Andrew, Perry Roland und Ichiro Fujinaga.</hi> 2011. „The Music Encoding
                    </bibl>
                    <bibl style="text-align: left; ">Initiative as a Document-Encoding Framework.“ In 
                        <hi rend="italic">ISMIR</hi>, 293–298. https://doi.org/10.5281/zenodo.1417609
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Kepper, Johannes und Daniel Röwenstrunk.</hi> 2007. „Das Edirom-Projekt. Werkzeuge für digitale Formen wissenschaftlich-kritischer Musikeditionen“. 
                        <hi rend="italic">Forum Musikbibliothek</hi> 28: 36–49.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Khulusi, Richard, Jakob Kusnick, Christofer Meinecke, Christina Gillmann, Josef Focht und Stefan Jänicke</hi>. 2020. „A Survey on Visualizations for Musical Data“. In 
                        <hi rend="italic">Computer Graphics Forum</hi>, 39:82–110. https://doi.org/10.1111/cgf.13905
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Kim-Boyle, David.</hi> 2022. „The Twittering Machine“. In 
                        <hi rend="italic">Proceedings of the International Conference on Technologies for Music Notation and Representation – TENOR’2022</hi>
                        <hi rend="bold" xml:space="preserve">, </hi>hg. von Vincent Tiffon, Jonathan Bell und Charles de Paiva Santana, 15–21. Marseille, France: PRISM Laboratory.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Maxwell, John Turner</hi>. 1981. „Mockingbird: An interactive composer’s aid“. Diss., Massachusetts Institute of Technology.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Repenning, Alexander, Jürg Zurmühle, Anna Lamprou und Daniel Hug.</hi> 2020. „Computational Music Thinking Patterns: Connecting Music Education with Computer Science Education through the Design of Interactive Notations“. In 
                        <hi rend="italic">Proceedings of the 12th International Conference on Computer Supported Education</hi> - Volume 1: CSME, 641–652. INSTICC, SciTePress. http://dx.doi.org/10.5220/0009817506410652
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Santini, Giovanni.</hi> 2022. „Linear: A Multi-Device Augmented Reality Environment for Interactive Notation and Music Improvisation
                        <hi rend="italic">“. In Proceedings of the International Conference on Technologies for Music Notation and Representation – TENOR’2022</hi>, hg. von Vincent Tiffon, Jonathan Bell und Charles de Paiva Santana, 37–43. Marseille, France: PRISM Laboratory, 2022. isbn: 979-10-97498-03-0.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Shvets, Anna, und Samer Darkazanli.</hi> 2022. „Conditional Semantic Music Generation in a Context of VR Project “Graphs in Harmony Learning”“. In 
                        <hi rend="italic">Proceedings of the International Conference on Technologies for Music Notation and Representation – TENOR’2022</hi>
                        <hi rend="bold" xml:space="preserve">, </hi>hg. von Vincent Tiffon, Jonathan Bell und Charles de Paiva Santana, 62–69. Marseille, France: PRISM Laboratory.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Waloschek, Simon.</hi> 2017. „Audioschnitt in digitalen Noten“. In 
                        <hi rend="italic">INFORMATIK 2017, 47. Jahrestagung der Gesellschaft für Informatik</hi>
                        <hi rend="bold">,</hi> hg. von M. Eibl und M. Gaedke. LNI. Chemnitz, Germany: Chemnitz University of Technology, Gesellschaft für Informatik, GI. https://dx.doi.org/10.18420/in2017_13
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Waloschek, Simon, Aristotelis Hadjakos und Alexander Pacha.</hi> 2019. „Identification and Cross-Document Alignment of Measures in Music Score Images“. In 
                        <hi rend="italic">Proc. of the 20th Int. Society for Music Information Retrieval Conf. (ISMIR).</hi> Delft, The Netherlands: Int. Society for Music Information Retrieval. https://doi.org/10.5281/zenodo.3527760
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
