<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:id="LÜCK_Christian_Standoff_Tools___Generische_Dienste_für_die_a" xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title type="full">
                    <title type="main">Standoff-Tools - Generische Dienste für die automatische Annotation von XML-Dokumenten mit Plain-Text-Werkzeugen</title>
                    <title type="sub"/>
                </title>
                <author>
                    <persName>
                        <surname>Lück</surname>
                        <forename>Christian</forename>
                    </persName>
                    <affiliation>Westfälische Wilhelmsuniversität Münster, Deutschland</affiliation>
                    <email>christian.lueck@uni-muenster.de</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2022-12-16T00:01:46.155091717</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                    <publisher>Culture and Computation Lab</publisher>
                    <address>
                        <addrLine>Université du Luxembourg</addrLine>
                        <addrLine>2, Avenue de l'Université</addrLine>
                        <addrLine>L-4365 Esch-sur Alzette</addrLine>
                        <addrLine>Luxembourg</addrLine>
                    </address>
                    <publisher>Luxembourg Centre for Contemporary and Digital History</publisher>
                    <address>
                        <addrLine>Université du Luxembourg</addrLine>
                        <addrLine>2, Avenue de l'Université</addrLine>
                        <addrLine>L-4365 Esch-sur Alzette</addrLine>
                        <addrLine>Luxembourg</addrLine>
                    </address>
                    <publisher>Trier Center for Digital Humanities</publisher>
                    <address>
                        <addrLine>Universität Trier</addrLine>  
                        <addrLine>Universitätsring 15</addrLine>
                        <addrLine>54296 Trier</addrLine>
                        <addrLine>Deutschland</addrLine>
                    </address>
                </publicationStmt>
            <sourceDesc>
                <p>Converted from an OASIS Open Document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Posterpräsentation</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>TEI</term>
                    <term>plain text</term>
                    <term>Annotationen</term>
                    <term>stand-off annotations</term>
                    <term>automatische Annotationen</term>
                    <term>computational methods</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>Programmierung</term>
                    <term>Annotieren</term>
                    <term>Bereinigung</term>
                    <term>Text</term>
                    <term>Werkzeuge</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>TEI-XML ist eine exzellente Technologie für digitale Editionen. Aber für die computationelle Analyse ist ein Korpus von XML-Dokumenten keine günstige Grundlage, weil Algorithmen erheblich komplexer werden, wenn statt linearem plain text ein XML-Baum durchlaufen werden muss. Zwar ist die Extraktion von plain text aus XML nicht aufwendig, aber für die Rückführung von Analyse-Ergebnissen in den XML-Baum gibt es bislang keine allgemeine Lösung. Dennoch ist eine solche in vielen Fällen wünschenswert, weil dann Analyse-Ergebnisse und die Struktur des Dokuments in einer allgemeinen Abfragesprache wie XQuery ausgewertet werden können. Es existieren Insellösungen zum Enrichment von TEI-XML, die auf bestimmte Tools ausgerichtet sind, etwa Spacy (Andorfer und Schlögl 2021, Meyer 2022), oder auf bestimmte Anwendungsfälle, etwa Alliterationen (Consalvi und Fumagalli 2022). Wünschenswert wäre jedoch, ganz allgemein Analyse-Tools für plain text entwickeln zu können und sie auch zum Enrichment von XML einsetzen zu können. Genau dies ermöglichen die hier vorgestellten 
                <hi rend="italic">StandOff Tools</hi>.
            </p>
            <div type="div1">
                <head>
                    <anchor type="bookmark-start" xml:id="id_komponenten-für-annotationspipelines"/>Komponenten für Annotationspipelines
                    <ptr type="bookmark-end" target="#id_komponenten-für-annotationspipelines"/>
                </head>
                <p>Die 
                    <hi rend="italic">StandOff Tools</hi> bestehen aus zwei Komponenten:<ref n="1" target="ftn1"/> dem Extraktor 
                    <hi rend="italic">E</hi> und dem Internalizer 
                    <hi rend="italic">I</hi>. In einer Annotationspipeline ist zwischen 
                    <hi rend="italic">E</hi> und 
                    <hi rend="italic">I</hi> ein anwendungsspezifischer Plain-Text-Tagger 
                    <hi rend="italic">T</hi> geschaltet. (Abb. 1) 
                    <hi rend="italic">E</hi> extrahiert aus dem XML-Quelldokument plain text, der an 
                    <hi rend="italic">T</hi> geleitet wird. 
                    <hi rend="italic">I</hi> nimmt von 
                    <hi rend="italic">T</hi> gelieferte Referenzierungen von Plain-Text-Fragmenten entgegen und bringt sie als Inline-Markup stets so in den XML-Baum ein, dass wohlgeformtes XML herauskommt.
                </p>
                <p>
                    <hi rend="italic">E</hi> und 
                    <hi rend="italic">I</hi> sind insofern generische Komponenten, als dass ein beliebiger Tagger in eine Pipeline zum Enrichment von XML eingesetzt werden kann, solange er folgende Anforderungen erfüllt: a) Er muss plain text verarbeiten, b) er muss darin Textpassagen (ranges) per 
                    <hi rend="italic">character offsets</hi> referenzieren, genauer: nach den in RFC 5147, Abschnitt 2.1.1 und 2.2.2 beschriebenen Regeln für 
                    <hi rend="italic">character ranges</hi>.<ref n="2" target="ftn2"/> Die vom Tagger ausgegebenen Ranges dürfen einander umfassen oder überlappen, so dass auch komplexe Strukturen in der XML-Quelle ausgezeichnet werden können. Die Ranges werden von 
                    <hi rend="italic">I</hi> so zerschnitten, dass Überlappungen mit anderen Ranges und dem internen Markup der XML-Quelle aufgelöst werden (Splitting).
                </p>
                <p>
                    <figure>
                        <graphic url="Pictures/64d18574eae86cb43c3b00207678ce68.jpg"/>
                        <head><p>Abb. 1: Schema einer Annotationspipeline für TEI XML</p></head>
                    </figure>
                </p>
                
            </div>
            <div type="div1">
                <head>
                    <anchor type="bookmark-start" xml:id="id_ansätze"/>Ansätze
                    <ptr type="bookmark-end" target="#id_ansätze"/>
                </head>
                <p>Die Komponenten 
                    <hi rend="italic">E</hi> und 
                    <hi rend="italic">I</hi> müssen so aufeinander abgestimmt sein, dass 
                    <hi rend="italic">I</hi> hinreichend Informationen hat, um die Annotationen von 
                    <hi rend="italic">T</hi> in das XML-Dokument einzubauen. Neben Ansätzen, die ein Sprachmodell, und sei es auch nur eine Tokenisierung, einführen und insofern nicht generisch sind (Schopper 2021, Andorfer und Schlögel 2021), lassen sich in der DH-Software-Entwicklung zwei Ansätze beobachten.
                </p>
                <p>Nach dem ersten extrahiert 
                    <hi rend="italic">E</hi> die Textknoten und sammelt dabei Informationen darüber, wo Elemente anfangen und aufhören. Dies ist auf DOM-Ebene möglich. 
                    <hi rend="italic">I</hi> kann dann Blätter des DOM-Baums so ersetzen, dass an ihrer Stelle neue Teilbäume die (zerschnittenen) Annotationen darstellen. Diesen Ansatz verfolgen die Lösung von Meyer (2022) und der Prototyp von Lassner (2021). Sein Vorteil ist, dass gängige XML-Parser eingesetzt werden können. Sein Nachteil ist, dass manche Merkmale der serialisierten XML-Quelle nicht bewahrt werden können, weil sie nicht zur DOM-Spezifikation gehören: 
                    <hi rend="italic">character</hi> und 
                    <hi rend="italic">entity references</hi> sowie 
                    <hi rend="italic">whitespace</hi> in Tags.
                </p>
                <p>Die 
                    <hi rend="italic">StandOff Tools</hi> verfolgen einen anderen Ansatz: Hier speichert 
                    <hi rend="italic">E</hi> Informationen über die Position, die jedes Zeichen des extrahierten Textes im XML-Quelldokument gehabt hat (
                    <hi rend="italic">offset mapping</hi>). DOM-Manipulationen erfolgen nicht. Ein Parser ermittelt Positionsdaten aus dem XML-Quelldokument. Die Extraktion erfolgt dann im Wesentlichen durch Kopieren von Zeichenketten aus dem serialisiert vorliegenden XML-Quelldokument. 
                    <hi rend="italic">I</hi> kann aufgrund des 
                    <hi rend="italic">offset mapping</hi> die Positionsdaten des Taggers auf die XML-Quelle beziehen. Wie im ersten Ansatz zerschneidet 
                    <hi rend="italic">I</hi> die Annotationen. Das Splitting, das Herzstück des Algorithmus, basiert auf einer Auswertung der Positionsdaten (offsets), die der XML-Parser für das interne Markup und der Tagger 
                    <hi rend="italic">T</hi> für die Annotationen liefert. Anschließend werden aufgrund der Positionsdaten Portionen der XML-Quelle in die Ausgabe kopiert und neue Tags dazwischen gesetzt. Dieser Ansatz hat den Nachteil, dass kein üblicher XML-Parser eingesetzt werden kann. Aber er reproduziert diejenigen Merkmale der XML-Quelle, welche im ersten Ansatz verloren gehen.
                </p>
            </div>
            <div type="div1">
                <head>
                    <anchor type="bookmark-start" xml:id="id_einsatz-für-manuelle-annotationen"/>Einsatz für manuelle Annotationen
                    <ptr type="bookmark-end" target="#id_einsatz-für-manuelle-annotationen"/>
                </head>
                <p>
                    <hi rend="italic">I</hi> kann auch allein betrieben werden, um manuelle Annotationen, die Passagen eines XML-Dokuments per character offsets referenzieren, zu internalisieren und zu anschließend zu visualisieren oder auszuwerten. Solche Annotationen können z.B. als Web Annotations (OA-Ontology) realisiert werden. Wenn dabei allerdings wie in CATMA eine schlichte Extraktion von Textknoten aus HTML oder XML erfolgt, gehen dabei Informationen verloren, welche für eine Internalisierung der Annotationen erforderlich sind. Insofern bleibt die Internalisierung von CATMA-Annotationen in TEI noch Desiderat (Cayless 2019). Dennoch weisen die verschiedenen neuen Entwicklungsansätze einen Weg abseits vom nicht-funktionalen Standoff-Konzept der TEI (Banski 2010), auf welchem der Mehrwert sowohl von Standoff-Annotationen als auch von TEI realisierbar ist.
                </p>
            </div>
        </body>
        <back>
<div type="notes">
<note xml:id="ftn1" rend="footnote text" n="1">
                         Die Komponenten liegen derzeit als Kommandozeilen-Programme vor. Eine Implementierung als Webservices ist in Planung, wobei sich die Modellierung des Informationsflusses komplexer gestaltet, weil der Extraktor einen mehrfachen Output hat.
                    </note>
<note xml:id="ftn2" rend="footnote text" n="2">Derzeit steht als Eingabe-Format für den Internalizer CSV zur Verfügung. Eine Implementierung der Syntax von RFC 5147 ist geplant.</note></div>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliographie</head>
                    <bibl>
                        <hi rend="bold">Andorfer, Peter und Schlögl, Matthias. 2021.</hi> acdh-spacytei. In: KONDE Weißbuch. Hrsg. v. Helmut W. Klug unter Mitarbeit von Selina Galka und Elisabeth Steiner im HRSM Projekt “Kompetenznetzwerk Digitale Edition”. Aufgerufen am: 3.8.2022. Handle: hdl.handle.net/11471/562.50.2.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Banski, Piotr. 2010.</hi> “Why TEI stand-off annotation doesn’t quite work and why you might want to use it nevertheless.” Balisage: The Markup Conference 2010, 10.4242/BalisageVol5.Banski01
                    </bibl>
                    <bibl>
                        <hi rend="bold">Cayless, Hugh. 2019.</hi> “Implementing TEI Standoff Annotation in the browser.” Proceedings of Balisage: The Markup Conference 2019, no. 23, 10.4242/BalisageVol23.Cayless01
                    </bibl>
                    <bibl>
                        <hi rend="bold">Consalvi, A. und Fumagalli, S. 2022.</hi> “Alliteration: automatic identification and encoding.” 2022 TEI Conference. Conference abstract, Session 1A-2, (im Erscheinen)
                    </bibl>
                    <bibl>
                        <hi rend="bold">Lassner, David.</hi> “The Standoff Converter. A standoff-based approach to work on TEI documents in Python that connects the world of digital philology with NLP.” 2021 TEI Conference and Members’ Meeting.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Meier, Wolfgang. 2022.</hi> “Names sell. Named Entity Recognition in TEI Publisher”, https://e-editiones.org/blog/ (zugegriffen: 3. August 2022)
                    </bibl>
                    <bibl>
                        <hi rend="bold">Schopper, Daniel. 2021.</hi> xsl-tokenizer. In: KONDE Weißbuch. Hrsg. v. Helmut W. Klug unter Mitarbeit von Selina Galka und Elisabeth Steiner im HRSM Projekt “Kompetenznetzwerk Digitale Edition”. Aufgerufen am: 3.8.2022. Handle: hdl.handle.net/11471/562.50.216.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
